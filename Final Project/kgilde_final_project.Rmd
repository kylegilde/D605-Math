---
title: "DATA 605 FUNDAMENTALS OF COMPUTATIONAL MATHEMATICS" 
subtitle: "Final Project"
author: "Kyle Gilde"
date: "12/11/2017"
output: 
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    toc: true
    toc_depth: 1
---
<style type="text/css">
body{
  font-family: Helvetica;
  font-size: 14pt;
}
</style>
<body>

```{r knitr_options, echo=FALSE}
knitr::opts_chunk$set(
                      error = FALSE
                      ,message = FALSE
                      ,tidy = TRUE
                      ,cache = TRUE
                      )
```


```{r packages, echo=F, collapse=T} 
packages <- c("prettydoc",'dplyr', 'psych', 'knitr', 'ggplot2', 'MASS', 'car', 'MLmetrics', 'gvlma', 'broom') 

installed_and_loaded <- function(pkg){
  ### See if we need to install and load any packages ###
  #CODE SOURCE: https://gist.github.com/stevenworthington/3178163
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
}

#excute function and display the loaded packages
data.frame(loaded_packages=installed_and_loaded(packages))
```

Your final is due by the end of day on 12/27/2017.  You should post your solutions to your GitHub account.  You are also expected to make a short presentation during our last meeting (3-5 minutes) or post a recording to the board.  This project will show off your ability to understand the elements of the class. 

You are to register for Kaggle.com (free) and compete in the House Prices: Advanced Regression Techniques competition.  https://www.kaggle.com/c/house-prices-advanced-regression-techniques . 

I want you to do the following.

Pick one of the quantitative independent variables from the training data set (train.csv) , and define that variable as  X.   Pick SalePrice as the dependent variable, and define it as Y for the next analysis.   

#Probability.  

```{r get_data}


```


Calculate as a minimum the below probabilities a through c.  Assume the small letter "x" is estimated as the 1st quartile of the X variable, and the small letter "y" is estimated as the 2d quartile of the Y variable.  Interpret the meaning of all probabilities.  

```{r thresholds}
X <- train_df$X1stFlrSF
Y <- train_df$SalePrice 
XY <- cbind(X, Y)

#no NAs
sum(is.na(XY))

total_length <- nrow(XY)
x <- quantile(X, .25)
y <- median(Y)
```


a. $P(X>x | Y>y)$
```{r a}
Y_greater_y <- data.frame(subset(XY, Y > y))

len_Y_greater_y <- nrow(Y_greater_y)

X_greater_x <- subset(Y_greater_y, X > x)

len_X_greater_x <- nrow(X_greater_x)

(`P(X>x | Y>y)` <- len_X_greater_x / len_Y_greater_y)
```

b.  $P(X>x & Y>y)$

```{r b}

(`P(X>x & Y>y)` <- nrow(subset(XY, Y > y & X > x))/ total_length)
```

c.  $P(X<x | Y>y)$		
```{r c}

Y_greater_y <- data.frame(subset(XY, Y > y))

len_Y_greater_y <- nrow(Y_greater_y)

X_greater_x <- subset(Y_greater_y, X < x)

len_X_greater_x <- nrow(X_greater_x)

(`P(X<x | Y>y)` <- len_X_greater_x / len_Y_greater_y)
``` 


Does splitting the training data in this fashion make them independent? In other words, does  P(XY)=P(X)P(Y))?   Check mathematically, and then evaluate by running a Chi Square test for association.  You might have to research this.  

**No, they are not equal and are not independent.**

$P(XY) \neq P(X)P(Y))$
```{r d}
(`P(X)P(Y))` <- sum(X > x) / total_length * sum(Y > y) / total_length)

`P(X>x & Y>y)` == `P(X)P(Y))`
```




**Because the p-value is near zero, we reject the null hypothesis that X and Y have no association**


```{r chi}
(chi_input <- table(X > x, Y > y))
prop.table(chi_input)
(chi_results <- chisq.test(chi_input))
```

#Descriptive and Inferential Statistics. 

Provide univariate descriptive statistics and appropriate plots for both variables.   Provide a scatterplot of X and Y. 

```{r desc}
options(scipen = 999)
kable(t(round(describe(XY, quant = c(.25,.75)), 2)))

XY_df <- data.frame(XY)

a <- ggplot(XY_df, aes(X, Y))
a + geom_point()

#X Variable
hist(XY_df$X, freq = FALSE, breaks = 25, main = "Variable X")
xfit <- seq(min(XY_df$X), max(XY_df$X), by = 1)
yfit <- dnorm(xfit, mean(XY_df$X), sd(XY_df$X))
lines(xfit, yfit)

#Y Variable
hist(XY_df$Y, freq = FALSE, breaks = 30, main = "Variable Y")
xfit <- seq(min(XY_df$Y), max(XY_df$Y), by = 10)
yfit <- dnorm(xfit, mean(XY_df$Y), sd(XY_df$Y))
lines(xfit, yfit)

par(mfrow = c(2, 2))
XYmodel <- lm(Y ~ X, data = XY_df)
summary(XYmodel)
plot(XYmodel)

```

```{r sresid}

sresid <- studres(XYmodel)
hist(sresid, freq = FALSE, breaks = 25, main = "Distribution of Studentized Residuals")
xfit <- seq(min(sresid), max(sresid), length = 50)
yfit <- dnorm(xfit)
lines(xfit, yfit)
```

#Box-Cox Transformations

Transform both variables simultaneously using Box-Cox transformations.  You might have to research this.

```{r transform}
# univariate BC tranformation
# BC <- boxcox(Y ~ X, data = XY_df)
# BC_df <- data.frame(BC)
# (lambda <- BC_df$x[which.max(BC_df$y)])
# XY_df_transform <- data.frame(
#   X = XY_df$X,
#   Y = (XY_df$Y  ^ lambda - 1) / lambda
# )

(lambda_values <- powerTransform(XY_df))
lambda_X <- lambda_values$lambda[1]
lambda_Y <- lambda_values$lambda[2]

XY_df_transform <- data.frame(
  X = (XY_df$X^lambda_X - 1) / lambda_X,
  Y = (XY_df$Y^lambda_Y - 1) / lambda_Y
)

kable(t(round(describe(XY_df_transform, quant = c(.25,.75)), 2)))

a <- ggplot(XY_df_transform, aes(X, Y))
a + geom_point()

#X Variable
hist(XY_df_transform$X, freq = FALSE, breaks = 25, main = "Variable X")
xfit <- seq(min(XY_df_transform$X), max(XY_df_transform$X), by = .01)
yfit <- dnorm(xfit, mean(XY_df_transform$X), sd(XY_df_transform$X))
lines(xfit, yfit)

#Y Variable
hist(XY_df_transform$Y, freq = FALSE, breaks = 30, main = "Variable Y")
xfit <- seq(min(XY_df_transform$Y), max(XY_df_transform$Y), by = .01)
yfit <- dnorm(xfit, mean(XY_df_transform$Y), sd(XY_df_transform$Y))
lines(xfit, yfit)

par(mfrow = c(2, 2))
XYmodel_BC <- lm(Y ~ X, data = XY_df_transform)
summary(XYmodel_BC)
plot(XYmodel_BC)

```
```{r sresid2}

sresid <- studres(XYmodel_BC)
hist(sresid, freq = FALSE, breaks = 25, main = "Distribution of Studentized Residuals")
xfit <- seq(min(sresid), max(sresid), length = 50)
yfit <- dnorm(xfit)
lines(xfit, yfit)

```

https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation

http://rcompanion.org/handbook/I_12.html

http://www.statisticshowto.com/box-cox-transformation/

https://www.rdocumentation.org/packages/car/versions/2.1-6/topics/powerTransform

#Linear Algebra and Correlation.   

Using at least three untransformed variables, build a correlation matrix.  Invert your correlation matrix. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix.

```{r LA}
selected_vars <- subset(train_df, select = c(LotArea, X1stFlrSF, OpenPorchSF, SalePrice))

(cor_matrix <- cor(selected_vars))
(precision_matrix <- solve(cor_matrix))

round(cor_matrix %*% precision_matrix, 6)
round(precision_matrix %*% cor_matrix, 6)
```

http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software

#Calculus-Based Probability & Statistics. 

Many times, it makes sense to fit a closed form distribution to data.  For your non-transformed independent variable, location shift (if necessary)  it so that the minimum value is above zero.  Then load the MASS package and run fitdistr to fit a density function of your choice.  (See https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html ).  Find the optimal value of the parameters for this distribution, and then take 1000 samples from this distribution (e.g., rexp(1000, ??) for an exponential).  Plot a histogram and compare it with a histogram of your non-transformed original variable.   

```{r calculus}
set.seed(4)
X_fit_f <- fitdistr(X, "exponential")
hist(X, breaks = 30)


rand_samp <- rexp(1000, X_fit_f$estimate[[1]])
hist(rand_samp, breaks = 30)
```


#Modeling.  

Build some type of regression model and submit your model to the competition board.  Provide your complete model summary and results with analysis.  Report your Kaggle.com  user name and score.

```{r clean}


#load CSVs
#Per data dictionary, "NA" indicates the absense of the characteristic 
#and shouldn't be treated as R's NAs.
#changed 2 Likert scales to categorical variables
train_csv <- read.csv('train.csv', 
                     na.strings = "",
                     colClasses=c("MSSubClass" = "factor",
                                  "MoSold" = "factor",
                                  "YrSold" = "factor",
                                  "OverallQual" = "factor",
                                  "OverallCond" = "factor"
                                  )
                     )

test_csv <- read.csv('test.csv', 
                    na.strings = "",
                    colClasses=c("MSSubClass" = "factor",
                                 "MoSold" = "factor",
                                 "YrSold" = "factor",
                                 "OverallQual" = "factor",
                                 "OverallCond" = "factor"                              
                                 )
                    )

current_year <- as.integer(format(Sys.Date(), "%Y"))

#mutate variables
train_df <- 
  train_csv %>% 
  mutate(
    years_since_built = current_year - YearBuilt,
    years_since_remodel = current_year - YearRemodAdd,
    years_since_garage_built = current_year - as.integer(GarageYrBlt), 
    MasVnrArea = as.integer(MasVnrArea),
    LotFrontage = as.integer(LotFrontage)
  ) %>% 
    dplyr::select(-c(YearBuilt, YearRemodAdd, GarageYrBlt))

test_df <- 
  test_csv %>% 
  mutate(
    years_since_built = current_year - YearBuilt,
    years_since_remodel = current_year - YearRemodAdd,
    years_since_garage_built = current_year - as.integer(GarageYrBlt)
  ) %>% 
  mutate_at(vars(MasVnrArea, LotFrontage, BsmtUnfSF, TotalBsmtSF, BsmtFinSF1, GarageArea, BsmtFinSF2),
           as.integer) %>% 
  dplyr::select(-c(YearBuilt, YearRemodAdd, GarageYrBlt))

inspect_data <- function(df){
  df_len <- nrow(df)
  NA_ct = as.vector(rapply(df, function(x) sum(is.na(x))))
  var_names <- names(df)
  
  df_metadata <- data.frame(
    position = 1:length(var_names),
    var_names = var_names,
    NA_ct = NA_ct,
    NA_percent = round(NA_ct / df_len, 4),
    class_type = rapply(df, class),
    uniq_value_ct = rapply(df,function(x)length(unique(x))),
    uniq_values = rapply(df, function(x) paste(sort(unique(x)), collapse = ','))
  )
  metrics <- data.frame(describe(df))
  factor_indices <- which(df_metadata$class_type == "factor")
  metrics[factor_indices, ] <- ''
  metrics <- metrics[, 2:ncol(metrics)]
  df_summary <- cbind(df_metadata, metrics)
  return(df_summary)
}
df_summary <- inspect_data(train_df)
df_summary <- inspect_data(test_df)
View(df_summary)
#write.csv(df_summary, file = "train_summary.csv")




#Remove 2 outliers due to being partial sales of homes
plot(SalePrice ~ GrLivArea, train_df)
train_df <- subset(train_df, !(GrLivArea > 4000 & SalePrice < 200000))


```

https://www.theanalysisfactor.com/likert-scale-items-as-predictor-variables-in-regression/
https://www.researchgate.net/post/how_independent_variables_measured_on_likert_scale_should_be_treated_in_binary_logistic_regression_as_continuous_variables_or_ordinal_variables
https://www.researchgate.net/post/Is_a_Likert-type_scale_ordinal_or_interval_data
https://stats.stackexchange.com/questions/121907/using-years-when-calculating-linear-regression



```{r baseline_model}

all_vars <- names(train_df)
remove <- c("SalePrice", "Id")
independent_vars <- setdiff(all_vars, remove)
independent_vars <- paste(independent_vars, collapse = "+")

options(max.print=2500)

(lm_formula <- as.formula(paste0('SalePrice ~', independent_vars)))

model1 <- lm(lm_formula, data = train_df)

model_sum <- summary(model1)
metrics_glance <- glance(model1)

model_summary <- function(model, model_sum, metrics_glance){
  results <- data.frame(
    n_vars = length(names(model$model)),
    RootMSLE = RMSLE(model$fitted.values, model$model$SalePrice),
    adj.r.sq = model_sum$adj.r.squared,
    f.statistic = paste(round(model_sum$fstatistic, 4), collapse = ", "),
    p.value = metrics_glance$p.value
  )
  return(results)
}
baseline_results <- model_summary(model1, model_sum, metrics_glance)
baseline_results
all_results <- baseline_results



```



```{r stepwise_selection}

#if(!exists("AIC_model")) 
AIC_model <- stepAIC(model1)

model_sum <- summary(AIC_model)
metrics_glance <- glance(AIC_model)
AIC_results <- model_summary(AIC_model, model_sum, metrics_glance)

plot(AIC_model)
#coef(model_sum)[, "Pr(>|t|)"]

all_results <- rbind(all_results, AIC_results)
print(all_results)
```


```{r}
scores_file <- "house-prices-advanced-regression-techniques-publicleaderboard.csv"
scores <- read.csv(scores_file)
scores$SubmissionDate <- as.Date(scores$SubmissionDate)
t(describe(subset(scores, SubmissionDate > "2010-12-01", select = Score), quant = c(.25, .1, .05, .01, .001)))
```


</body>