---
title: "DATA 605 FUNDAMENTALS OF COMPUTATIONAL MATHEMATICS" 
subtitle: "Final Project"
author: "Kyle Gilde"
date: "12/11/2017"
output: 
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    toc: true
    toc_depth: 2
---
<style type="text/css">
body{
  font-family: Helvetica;
  font-size: 14pt;
}
</style>
<body>

```{r knitr_options, echo=FALSE}
knitr::opts_chunk$set(
                      error = FALSE
                      ,message = FALSE
                      ,tidy = TRUE
                      ,cache = TRUE
                      )
```


```{r packages, echo=F, collapse=T} 
required_packages <- c("prettydoc",'dplyr', 'psych', 'knitr', 'ggplot2', 'MASS', 'car', 'MLmetrics', 'broom', 'randomForest', 'e1071', 'forcats', 'mice', 'missForest') 

### See if we need to install and load any packages ###
new_packages <- required_packages[!(required_packages %in% installed.packages())]
if (length(new_packages)) install.packages(new_packages, dependencies = TRUE)
data.frame(loaded_packages = sapply(required_packages, require, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE))

```

Your final is due by the end of day on 12/20/2017.  You should post your solutions to your GitHub account.  You are also expected to make a short presentation during our last meeting (3-5 minutes) or post a recording to the board.  This project will show off your ability to understand the elements of the class. 

You are to register for Kaggle.com (free) and compete in the House Prices: Advanced Regression Techniques competition.  https://www.kaggle.com/c/house-prices-advanced-regression-techniques . 

I want you to do the following.

Pick one of the quantitative independent variables from the training data set (train.csv) , and define that variable as  X.   Pick SalePrice as the dependent variable, and define it as Y for the next analysis.   

#Probability.  

```{r get_data}
#load CSVs
#changed 2 Likert scales & 2 date-component values to categorical variables
#https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt

train_csv <- read.csv('train.csv', 
                     colClasses=c("MSSubClass" = "factor",
                                  "MoSold" = "factor",
                                  "YrSold" = "factor",
                                  "OverallQual" = "factor",
                                  "OverallCond" = "factor"
                                  )
                     )

test_csv <- read.csv('test.csv', 
                    colClasses=c("MSSubClass" = "factor",
                                 "MoSold" = "factor",
                                 "YrSold" = "factor",
                                 "OverallQual" = "factor",
                                 "OverallCond" = "factor"                              
                                 )
                    )

```

https://www.theanalysisfactor.com/likert-scale-items-as-predictor-variables-in-regression/
https://www.researchgate.net/post/how_independent_variables_measured_on_likert_scale_should_be_treated_in_binary_logistic_regression_as_continuous_variables_or_ordinal_variables
https://www.researchgate.net/post/Is_a_Likert-type_scale_ordinal_or_interval_data
https://stats.stackexchange.com/questions/121907/using-years-when-calculating-linear-regression


Calculate as a minimum the below probabilities a through c.  Assume the small letter "x" is estimated as the 1st quartile of the X variable, and the small letter "y" is estimated as the 2d quartile of the Y variable.  Interpret the meaning of all probabilities.  

```{r thresholds}
X <- train_csv$X1stFlrSF
Y <- train_csv$SalePrice 
XY <- cbind(X, Y)

total_length <- nrow(XY)
x <- quantile(X, .25)
y <- median(Y)
```


a. $P(X>x | Y>y)$
```{r a}
Y_greater_y <- data.frame(subset(XY, Y > y))

len_Y_greater_y <- nrow(Y_greater_y)

X_greater_x <- subset(Y_greater_y, X > x)

len_X_greater_x <- nrow(X_greater_x)

(`P(X>x | Y>y)` <- len_X_greater_x / len_Y_greater_y)
```

b.  $P(X>x & Y>y)$

```{r b}

(`P(X>x & Y>y)` <- nrow(subset(XY, Y > y & X > x))/ total_length)
```

c.  $P(X<x | Y>y)$		
```{r c}

Y_greater_y <- data.frame(subset(XY, Y > y))

len_Y_greater_y <- nrow(Y_greater_y)

X_greater_x <- subset(Y_greater_y, X < x)

len_X_greater_x <- nrow(X_greater_x)

(`P(X<x | Y>y)` <- len_X_greater_x / len_Y_greater_y)
``` 


Does splitting the training data in this fashion make them independent? In other words, does  P(XY)=P(X)P(Y))?   Check mathematically, and then evaluate by running a Chi Square test for association.  You might have to research this.  

**No, they are not equal and are not independent.**

$P(XY) \neq P(X)P(Y))$
```{r d}
(`P(X)P(Y))` <- sum(X > x) / total_length * sum(Y > y) / total_length)

`P(X>x & Y>y)` == `P(X)P(Y))`
```




**Because the p-value is near zero, we reject the null hypothesis that X and Y have no association**


```{r chi}
(chi_input <- table(X > x, Y > y))
prop.table(chi_input)
(chi_results <- chisq.test(chi_input))
```

#Descriptive and Inferential Statistics. 

Provide univariate descriptive statistics and appropriate plots for both variables.   Provide a scatterplot of X and Y. 

```{r desc}
options(scipen = 999)
kable(t(round(describe(XY, quant = c(.25,.75)), 2)))

XY_df <- data.frame(XY)

a <- ggplot(XY_df, aes(X, Y))
a + geom_point()

#X Variable
hist(XY_df$X, freq = FALSE, breaks = 25, main = "Variable X")
xfit <- seq(min(XY_df$X), max(XY_df$X), by = 1)
yfit <- dnorm(xfit, mean(XY_df$X), sd(XY_df$X))
lines(xfit, yfit)

#Y Variable
hist(XY_df$Y, freq = FALSE, breaks = 30, main = "Variable Y")
xfit <- seq(min(XY_df$Y), max(XY_df$Y), by = 10)
yfit <- dnorm(xfit, mean(XY_df$Y), sd(XY_df$Y))
lines(xfit, yfit)

XYmodel <- lm(Y ~ X, data = XY_df)
summary(XYmodel)
plot(XYmodel, which = 2)
```

```{r sresid}

sresid <- studres(XYmodel)
hist(sresid, freq = FALSE, breaks = 25, main = "Distribution of Studentized Residuals")
xfit <- seq(min(sresid), max(sresid), length = 50)
yfit <- dnorm(xfit)
lines(xfit, yfit)
```

#Box-Cox Transformations

Transform both variables simultaneously using Box-Cox transformations.  You might have to research this.

```{r transform}
(lambda_values <- powerTransform(XY_df))
lambda_X <- lambda_values$lambda[1]
lambda_Y <- lambda_values$lambda[2]

XY_df_transform <- data.frame(
  X = (XY_df$X^lambda_X - 1) / lambda_X,
  Y = (XY_df$Y^lambda_Y - 1) / lambda_Y
)
summary(powerTransform(XY_df))
kable(t(round(describe(XY_df_transform, quant = c(.25,.75)), 2)))

a <- ggplot(XY_df_transform, aes(X, Y))
a + geom_point()

#X Variable
hist(XY_df_transform$X, freq = FALSE, breaks = 25, main = "Variable X")
xfit <- seq(min(XY_df_transform$X), max(XY_df_transform$X), by = .01)
yfit <- dnorm(xfit, mean(XY_df_transform$X), sd(XY_df_transform$X))
lines(xfit, yfit)

#Y Variable
hist(XY_df_transform$Y, freq = FALSE, breaks = 30, main = "Variable Y")
xfit <- seq(min(XY_df_transform$Y), max(XY_df_transform$Y), by = .01)
yfit <- dnorm(xfit, mean(XY_df_transform$Y), sd(XY_df_transform$Y))
lines(xfit, yfit)

XYmodel_BC <- lm(Y ~ X, data = XY_df_transform)

plot(XYmodel_BC, which = 2)

```


```{r sresid2}

sresid <- studres(XYmodel_BC)
hist(sresid, freq = FALSE, breaks = 25, main = "Distribution of Studentized Residuals")
xfit <- seq(min(sresid), max(sresid), length = 50)
yfit <- dnorm(xfit)
lines(xfit, yfit)

```

https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation

http://rcompanion.org/handbook/I_12.html

http://www.statisticshowto.com/box-cox-transformation/

https://www.rdocumentation.org/packages/car/versions/2.1-6/topics/powerTransform

#Linear Algebra and Correlation.   

Using at least three untransformed variables, build a correlation matrix.  Invert your correlation matrix. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix.

```{r LA}
selected_vars <- subset(train_csv, select = c(LotArea, X1stFlrSF, OpenPorchSF, SalePrice))

(cor_matrix <- cor(selected_vars))
(precision_matrix <- solve(cor_matrix))

round(cor_matrix %*% precision_matrix, 6)
round(precision_matrix %*% cor_matrix, 6)
```

http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software

#Calculus-Based Probability & Statistics. 

Many times, it makes sense to fit a closed form distribution to data.  For your non-transformed independent variable, location shift (if necessary)  it so that the minimum value is above zero.  Then load the MASS package and run fitdistr to fit a density function of your choice.  (See https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html ).  Find the optimal value of the parameters for this distribution, and then take 1000 samples from this distribution (e.g., rexp(1000, ??) for an exponential).  Plot a histogram and compare it with a histogram of your non-transformed original variable.   

```{r calculus}
set.seed(4)
X_fit_f <- fitdistr(X, "exponential")
hist(X, breaks = 30)


rand_samp <- rexp(1000, X_fit_f$estimate[[1]])
hist(rand_samp, breaks = 30)
```


#Modeling.  

Build some type of regression model and submit your model to the competition board.  Provide your complete model summary and results with analysis.  Report your Kaggle.com  user name and score.


##Clean Data
```{r clean}
#combine test & training
test_csv$SalePrice <- NA
all_data <- rbind(train_csv, test_csv)

#mutate variables in train and test data sets
current_year <- as.integer(format(Sys.Date(), "%Y"))

all_data_cleaned <- all_data %>% 
  #change the actual year values to age in unit years
  mutate(
    years_since_built = current_year - YearBuilt,
    years_since_remodel = current_year - YearRemodAdd,
    years_since_garage_built = current_year - as.integer(GarageYrBlt)
  ) %>%
  #Per the data dictionary, the following variables have meaningful NA values
  mutate_at(
    vars(Alley, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, PoolQC, Fence, MiscFeature),
    fct_explicit_na, na_level = "None"
  ) %>% 
  dplyr::select(-c(YearBuilt, YearRemodAdd, GarageYrBlt, Id))

#Remove 2 outliers due to being partial sales of homes
#https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt
plot(SalePrice ~ GrLivArea, all_data_cleaned)
all_data_cleaned <- subset(all_data_cleaned, !(GrLivArea > 4000 & SalePrice < 200000))


inspect_metadata <- function(df){
  ###Takes a data frame &
  ###Checks for NAs, incorrect class types, inspects the unique values
  ###Adds summary metrics
  df_len <- nrow(df)
  NA_ct = as.vector(rapply(df, function(x) sum(is.na(x))))
  var_names <- names(df)
  
  df_metadata <- data.frame(
    position = 1:length(var_names),
    var_names = var_names,
    NA_ct = NA_ct,
    NA_percent = round(NA_ct / df_len, 4),
    class_type = rapply(df, class),
    uniq_value_ct = rapply(df,function(x)length(unique(x))),
    uniq_values = rapply(df, function(x) paste(sort(unique(x)), collapse = ','))
  )
  metrics <- data.frame(describe(df))
  factor_indices <- which(df_metadata$class_type == "factor")
  metrics[factor_indices, ] <- ''
  metrics <- metrics[, 2:ncol(metrics)]
  df_summary <- cbind(df_metadata, metrics)
  return(df_summary)
}

#View(df_metadata <- inspect_metadata(test_df))

#Impute missing values
imputed_Data <- missForest(all_data_cleaned)

#check imputed values
imputed_Data$ximp

#check imputation error
imputed_Data$OOBerror

# numeric_vars <- subset(df_metadata, class_type != 'factor', select = var_names)[, 1]
# numeric_vars <- setdiff(numeric_vars, c('Id', 'SalePrice'))
# 
# numeric_data <- subset(all_data_cleaned, select = numeric_vars)
# other_data <- all_data_cleaned[, !names(all_data_cleaned) %in% numeric_vars]
# 
# imputed_Data <- mice(numeric_data, m=5, maxit = 50, method = 'pmm', seed = 500)
# summary(imputed_Data)
# completed_numeric_data <- complete(imputed_Data, 1)

all_data_completed <- cbind(other_data, completed_numeric_data)

#Split test & training
train_df <- subset(na.omit(all_data_completed), Id <= nrow(train_csv))
test_df <- subset(all_data_completed, Id > nrow(train_csv), select = -SalePrice)
```

https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/

##Baseline Model
```{r baseline_model}
#Create baseline model with all the variables
baseline <- lm(SalePrice ~ ., data = train_df)
```

The model's plots show some violations of LR conditions. The residuals are not normally distributed or homoscedastic.

```{r baseline_results}
par(mfrow = c(2, 2)) 
plot(baseline)

model_summary <- function(model, y_var){
  ### Summarizes the model's key statistics in one row
  df_summary <- glance(summary(model))
  df_summary$n_vars <- ncol(model$model) - 1
  df_summary$RootMSLE = RMSLE(model$fitted.values, model$model[, y_var])
  df_summary$model_name <- deparse(substitute(model)) 
  df_summary <- subset(df_summary, select = c(model_name, n_vars, adj.r.squared, statistic, RootMSLE, p.value, sigma, df))
}
mod_sum <- model_summary(baseline, 'SalePrice')
kable(all_results <- mod_sum)
```



## Model with Transformations
```{r baseline_PT}
train_df_transformed <- baseline$model

PT <- powerTransform(as.formula(baseline$call), family="bcnPower", data = train_df_transformed)

train_df_transformed$SalePrice  <- bcnPower(train_df_transformed$SalePrice, 
                                            lambda = PT$lambda, 
                                            gamma = PT$gamma)

baseline_PT <- lm(SalePrice ~ ., data = train_df_transformed)
```
https://stats.stackexchange.com/questions/61217/transforming-variables-for-multiple-regression-in-r

The Q-Q plot isn't as normal as it could be, and there are a number of outliers.

```{r, baseline_PT_results}
par(mfrow = c(2, 2)) 
plot(baseline_PT)

mod_sum <- model_summary(baseline_PT, 'SalePrice')

kable(all_results <- rbind(all_results, mod_sum))
```


##AIC Stepwise Model
```{r stepwise_selection, results='hide'}
#if(!exists("AIC_model")) 
  AIC_model <- stepAIC(baseline)
```

The Q-Q plot isn't normal, and there are a number of outliers.

```{r, stepwise_results}
par(mfrow = c(2, 2)) 
plot(AIC_model)

mod_sum <- model_summary(AIC_model, 'SalePrice')
kable(all_results <- rbind(all_results, mod_sum))

# cooksd <- cooks.distance(AIC_model)
# cooks_threshold <-  4 * mean(cooksd, na.rm=T)
# 
# plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
# abline(h = cooks_threshold, col="red")  # add cutoff line
# text(x = 1:length(cooksd)+1,
#      y = cooksd,
#      labels=ifelse(cooksd > cooks_threshold, names(cooksd),""),
#      col="red")  # add labels
# 
# length(cooksd[cooksd > cooks_threshold])
```

##AIC Stepwise Model with Transformations
```{r, AIC_model_PT, results='hide'}

train_df_AIC_PT <- AIC_model$model
PT <- powerTransform(as.formula(AIC_model$call), family="bcnPower", data = train_df_AIC_PT)
summary(PT)

train_df_AIC_PT$SalePrice  <- bcnPower(train_df_AIC_PT$SalePrice, 
                                            lambda = PT$lambda, 
                                            gamma = PT$gamma)

AIC_model_PT <- lm(SalePrice ~ ., data = train_df_AIC_PT)

#if(!exists("AIC_model_PT")) 
  AIC_model_PT <- stepAIC(AIC_model_PT)

```


```{r, AIC_model_PT_results}
par(mfrow = c(2, 2)) 
plot(AIC_model_PT)

mod_sum <- model_summary(AIC_model_PT, 'SalePrice')
kable(all_results <- rbind(all_results, mod_sum))

cols_needed <- intersect(names(AIC_model_PT$model), names(test_df))

fit <- predict(AIC_model_PT, newdata = subset(test_df, select = cols_needed))
summary(fit)

table(all_data_completed$OverallQual)
table(AIC_model_PT$model$OverallQual)
table(test_df$OverallQual)

#Outlier analysis
# cooksd <- cooks.distance(AIC_model_PT)
# cooks_threshold <-  4 * mean(cooksd, na.rm=T)
# 
# plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
# abline(h = cooks_threshold, col="red")  # add cutoff line
# text(x = 1:length(cooksd)+1,
#      y = cooksd,
#      labels=ifelse(cooksd > cooks_threshold, names(cooksd),""),
#      col="red")  # add labels
# 
# length(cooksd[cooksd > cooks_threshold])

```


##Random Forest

```{r rforest}
y_train <- train_df$SalePrice
x_train_df <- subset(train_df, select = -SalePrice)

#fit <- rpart(y_train ~ ., data = x_train_df, method="class")
#fit <- randomForest(x_train_df, y = y_train, xtest = test_df, ntree=500, importance = T)
# 
# summary(fit)
# fit$importance
# fit$inbag
#Predict Output 
#predicted= predict(fit, x_test)

```


##Gradient Boosting


```{r}
scores_file <- "house-prices-advanced-regression-techniques-publicleaderboard.csv"
scores <- read.csv(scores_file)
scores$SubmissionDate <- as.Date(scores$SubmissionDate)
t(describe(subset(scores, SubmissionDate > "2010-12-01", select = Score), quant = c(.25, .1, .05, .01, .001)))
```


</body>